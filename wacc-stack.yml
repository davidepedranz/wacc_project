# please this command before deploying this stack:
# docker network create consul-net -d overlay --internal --subnet=172.20.0.0/24

version: "3.3"
services:

  consul:
    image: sdelrio/consul:latest
    labels: [app=consul]
    ports:
      - "8500:8500"
    volumes:
      - consul_data:/consul/data
    networks:
      - consul-net
      - internal
    environment:
      - 'CONSUL_LOCAL_CONFIG={"skip_leave_on_interrupt": true}'
      - CONSUL_BIND_INTERFACE=eth0
      - CONSUL=consul
      - CONSUL_CHECK_LEADER=true
    deploy:
      mode: global
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: continue

  registrator:
    image: gliderlabs/registrator:latest
    volumes:
      - /var/run/docker.sock:/tmp/docker.sock
    networks:
      - consul-net
    command: ["-internal", "consul://consul:8500"]

  traefik:
    image: traefik:alpine
    command: --web --docker --docker.swarmmode --docker.watch --docker.domain=${DOMAIN:-localhost} --docker.exposedbydefault=false --logLevel=INFO
    ports:
      - "80:80"
      - "8080:8080"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /dev/null:/traefik.toml
    networks:
      - public
      - internal
    deploy:
      mode: global
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: on-failure

  frontend:
    image: wacccourse/frontend:latest
    networks:
      - internal
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
      labels:
        - "traefik.enable=true"
        - "traefik.backend=frontend"
        - "traefik.port=80"
        - "traefik.frontend.rule=Host:${DOMAIN:-localhost}"

  backend:
    image: wacccourse/backend:latest
    networks:
      - internal
    environment:
      APPLICATION_SECRET: "some-fake-secret-to-change"
      MONGODB_URI: "mongodb://mongo:27017/wacc"
      CASSANDRA_URI: "cassandradb://cassandra:9042"
      DOCKER_HOST : "http://docker-socket-proxy:2375"
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
      labels:
        - "traefik.enable=true"
        - "traefik.backend=backend"
        - "traefik.port=9000"
        - "traefik.frontend.rule=Host:${DOMAIN:-localhost};PathPrefixStrip:/api"

  docker-socket-proxy:
    image: wacccourse/docker-socket-proxy:latest
    networks:
      - internal
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    deploy:
      mode: global
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: on-failure

  mongo:
    image: mongo:3.4
    networks:
      - internal
    volumes:
      - mongo_data:/data/db
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: on-failure
      labels:
        - "traefik.enable=false"

  cassandra:
    image: cassandra:3.0
    networks:
      - internal
    volumes:
      - cassandra_data:/cassandra/data/db
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: on-failure
      labels:
        - "traefik.enable=false"

  # Configuration for our seed cassandra node. The node is call DC1N1
  # .i.e Node 1 in Data center 1.
  DC1N1:
    # Cassandra image for Cassandra version 3.1.0. This is pulled
    # from the docker store.
    image: cassandra:3.0
    # In case this is the first time starting up cassandra we need to ensure
    # that all nodes do not start up at the same time. Cassandra has a
    # 2 minute rule i.e. 2 minutes between each node boot up. Booting up
    # nodes simultaneously is a mistake. This only needs to happen the firt
    # time we bootup. Configuration below assumes if the Cassandra data
    # directory is empty it means that we are starting up for the first
    # time.
    command: bash -c 'if [ -z "$$(ls -A /var/lib/cassandra/)" ] ; then sleep 0; fi && /docker-entrypoint.sh cassandra -f'
    # Network for the nodes to communicate
    networks:
        - internal
    # Maps cassandra data to a local folder. This preserves data across
    # container restarts. Note a folder n1data get created locally
    volumes:
        - cassandra_data1:/var/lib/cassandra
    # Docker constainer environment variable. We are using the
    # CASSANDRA_CLUSTER_NAME to name the cluster. This needs to be the same
    # across clusters. We are also declaring that DC1N1 is a seed node.
    environment:
        - CASSANDRA_CLUSTER_NAME=dev_cluster
        - CASSANDRA_SEEDS=DC1N1
    ulimits:
        memlock: -1
        nproc: 32768
        nofile: 100000
  # This is configuration for our non seed cassandra node. The node is call
  # DC1N1 .i.e Node 2 in Data center 1.
  DC1N2:
    # Cassandra image for Cassandra version 3.1.0. This is pulled
    # from the docker store.
    image: cassandra:3.0
    # In case this is the first time starting up cassandra we need to ensure
    # that all nodes do not start up at the same time. Cassandra has a
    # 2 minute rule i.e. 2 minutes between each node boot up. Booting up
    # nodes simultaneously is a mistake. This only needs to happen the firt
    # time we bootup. Configuration below assumes if the Cassandra data
    # directory is empty it means that we are starting up for the first
    # time.
    command: bash -c 'if [ -z "$$(ls -A /var/lib/cassandra/)" ] ; then sleep 60; fi && /docker-entrypoint.sh cassandra -f'
    # Network for the nodes to communicate
    networks:
        - internal
    # Maps cassandra data to a local folder. This preserves data across
    # container restarts. Note a folder n1data get created locally
    volumes:
        - cassandra_data2:/var/lib/cassandra
    # Docker constainer environment variable. We are using the
    # CASSANDRA_CLUSTER_NAME to name the cluster. This needs to be the same
    # across clusters. We are also declaring that DC1N1 is a seed node.
    environment:
        - CASSANDRA_CLUSTER_NAME=dev_cluster
        - CASSANDRA_SEEDS=DC1N1
    # Since DC1N1 is the seed node
    depends_on:
          - DC1N1
    ulimits:
        memlock: -1
        nproc: 32768
        nofile: 100000

  # This is configuration for our non seed cassandra node. The node is call
  # DC1N3 .i.e Node 3 in Data center 1.
  DC1N3:
    image: cassandra:3.0
    # In case this is the first time starting up cassandra we need to ensure
    # that all nodes do not start up at the same time. Cassandra has a
    # 2 minute rule i.e. 2 minutes between each node boot up. Booting up
    # nodes simultaneously is a mistake. This only needs to happen the firt
    # time we bootup. Configuration below assumes if the Cassandra data
    # directory is empty it means that we are starting up for the first
    # time.
    command: bash -c 'if [ -z "$$(ls -A /var/lib/cassandra/)" ] ; then sleep 120; fi && /docker-entrypoint.sh cassandra -f'
    # Network for the nodes to communicate. This is pulled from docker hub.
    networks:
        - internal
    # Maps cassandra data to a local folder. This preserves data across
    # container restarts. Note a folder n1data get created locally
    volumes:
        - cassandra_data3:/var/lib/cassandra
    # Docker constainer environment variable. We are using the
    # CASSANDRA_CLUSTER_NAME to name the cluster. This needs to be the same
    # across clusters. We are also declaring that DC1N1 is a seed node.
    environment:
        - CASSANDRA_CLUSTER_NAME= dev_cluster
        - CASSANDRA_SEEDS=DC1N1
    # Since DC1N1 is the seed node
    depends_on:
          - DC1N1
    # Exposing ports for inter cluste communication. Note this is already
    # done by the docker file. Just being explict about it.
    # Cassandra ulimt recommended settings
    ulimits:
        memlock: -1
        nproc: 32768
        nofile: 100000
  # A web based interface for managing your docker containers.
  portainer:
    image: portainer/portainer
    command: --templates http://templates/templates.json
    networks:
        - internal
    volumes:
        - docker_sock:/var/run/docker.sock
        - portainer_data:/data
    # Enable you to access potainers web interface from your host machine
    # using http://localhost:10001
    ports:
        - "10001:9000"

networks:
  public:
    driver: overlay
  internal:
    driver: overlay
    internal: true
  consul-net:
    external: true

volumes:
  consul_data: {}
  mongo_data: {}
  cassandra_data: {}
  cassandra_data1: {}
  cassandra_data2: {}
  cassandra_data3: {}
  docker_sock: {}
  portainer_data: {}
